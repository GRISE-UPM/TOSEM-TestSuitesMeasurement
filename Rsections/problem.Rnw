\SweaveInput{common.Rnw}

\section{Problem description}\label{sec:problem-description}

In this paper, we are using two replications conducted in the industry as running examples. These replications will be referred to as \textbf{PT} and \textbf{EC} to maintain companies' anonymity. PT and EC replications have been described in detail in \cite{Tosun2019} and \cite{Dieste2021}, respectively.

\subsection{Experimental replications}\label{sec:design}

PT and EC replications explore two programming strategies: TDD and incremental test-last development (ITLD). TDD requires writing tests before production code, whereas ITLD proceeds inversely. The experimental design is described in Table~\ref{tab:experimental-design}, where the programming strategy is a within-subjects factor. 

The programming strategies have been applied on two greenfield experimental tasks, namely Mars Rover API (MR\footnote{MR and BSK task specifications are included in \url{https://github.com/GRISE-UPM/TestSuitesMeasurement/tree/master/experimental_tasks}.\label{foot:web}}) \cite{mr} and Bowling Score Keeper (BSK\footnote{See footnote \ref{foot:web}.}) \cite{bsk}. MR and BSK are crossed across programming strategies to avoid confounding. This type of design is frequent in SE when participants need to receive specific training, and a few experimental subjects are available. 

The assignment of subjects to groups was performed randomly. 17 and 20 experimental subjects participated in PT and EC, respectively. They were programmers with different degrees of experience, employed in the corresponding companies. PT programmers used Java and jUnit, whereas EC ones used C++ and Boost Test.

\begin{table}[htb]
\small
\centering
\caption{Experimental design}
\label{tab:experimental-design}
\begin{tabular}{llll}
 &  & \multicolumn{2}{c}{\textbf{Treatment}} \\ \cline{3-4} 
 &  & (Session 1) & (Session 2) \\
 &  & ITLD & TDD \\ \cline{2-4} 
\multirow{2}{*}{\textbf{Group}} & Group $MR \rightarrow BSK$ & MR & BSK \\
 & Group $BSK \rightarrow MR$ & BSK & MR \\ \cline{2-4} 
\end{tabular}
\end{table}

\subsection{Response variables and measurement procedure}

We studied external quality (QLTY) and productivity (PROD) response variables. QLTY represents the software quality measured in terms of compliance with the software requirements. Similarly, PROD represents the amount of functionality delivered by programmers. These response variables have been frequently explored in TDD research, e.g., \cite{Causevic2012,Desai2009,Erdogmus2005,Fucci2013}, as well as in our previous research, e.g., \cite{Tosun2016,Dieste2017,Tosun2019}.

QLTY and PROD were measured using specifically designed \textit{unit tests suites}\footnote{The test suites are provided as a single Eclipse workspace containing four projects. They are available as one Eclipse workspace at \url{https://github.com/GRISE-UPM/TestSuitesMeasurement/tree/master/test_suites}.}. One test suite was reused from previous experiments \cite{Erdogmus2005,Tosun2016}. It was generated using an \textit{ad-hoc} (AH) strategy and coded in jUnit. Ad-hoc means that a formal procedure to create the test cases has not been used; the authors of the test suites (MR and BSK) applied their best judgment to derive a set of test cases from the functional requirements.

To avoid the mono-method threat to validity \cite[81-82]{Shadish2002}, we designed new test suites (for MR and BSK) using the \textit{equivalence partitioning} (EP) technique, and coded them in jUnit. We applied equivalence partitioning according to \cite[Chapter 4]{Myers2011}; details can be found in \cite{Elizabeth2015}.  Later, both test suites were ported to Boost Test.

The test suites give a percentage (0\%-100\%) as a result. For instance, in the case of QLTY, this percentage represents the degree to which the code complies with the software requirements: A 0\% value means that the code does not satisfy any requirement; a 100\% value means that the code satisfies all requirements. 

\subsection{Characteristics of the MR and BSK test suites}

The AH and EP test suites are composed of a varying number of test classes/methods/assertions, as indicated in Table \ref{tab:characteristics}. BSK's requirements are well defined, hence the AH and EP test suites exhibit strong similarities: They have the same number of test classes (which are roughly equivalent to functional requirements), and a comparable number of assertions. The EP technique provides a perfect correspondence between test methods and assertions.

MR is defined at a high level and misses a stable specification. Consequently, the AH and EP test suites diverge considerably. However, \textit{divergence} does not imply \textit{measurement differences}. The same code can be measured using different test suites and obtain the same measurement results. For instance, the function 
\lstinline!int sum(int a, int b){ return a + b; }! 
gets a 100\% QLTY with both test suites below:
\begin{lstlisting}[caption={Equivalent tests suites, from the measurement viewpoint},label={lst:equivalent}]
public class Suite1{
	@Test
	public void testOnePlusOneGivesTwo() {
	     assertEquals(2, sum(1, 1)); }
}

public class Suite2{
	@Test
	public void testThreePlusTwoGivesFive() {
	     assertEquals(500, sum(300, 200)); }

	@Test
	public void testThreePlusMinusTwoGivesOne() {
	     assertEquals(300, sum(400, -100)); }
}
\end{lstlisting}



\begin{table}[]
\centering
\small
\caption{Characteristics of the AH and EP test suites}
\label{tab:characteristics}
\begin{tabular}{lllcc}
 &  & \textbf{} & \multicolumn{2}{c}{\textbf{Test suite}} \\ \cline{4-5} 
 &  &  & AH & EP \\ \cline{2-5} 
\multirow{6}{*}{\textbf{Task}} & \multirow{3}{*}{MR} & Test classes & 11 & 9 \\
 &  & Test methods & 52 & 32 \\
 &  & Assertions & 89 & 32 \\ \cline{2-5} 
 & \multirow{3}{*}{BSK} & Test classes & 13 & 13 \\
 &  & Test methods & 48 & 72 \\
 &  & Assertions & 55 & 72 \\ \cline{2-5} 
\end{tabular}
\end{table}

From a testing perspective, the AH and EP test suites are largely equivalent. When we exercise the test suites on correct implementations of MR and BSK, the coverage is almost identical, as shown in Table~\ref{tab:coverage}. Statement coverage is virtually 100\% in all cases. Branch coverage is somewhat smaller but exceeds 90\% (except the AH test suite when applied to the MR task, which has an 88\% branch coverage). 

\begin{table}[]
\small
\centering
\caption{Coverage of the AH and EP test suites}
\label{tab:coverage}
\begin{tabular}{lllrr}
 &  &  & \multicolumn{2}{c}{\textbf{Test suite}} \\ \cline{4-5} 
 &  &  & \multicolumn{1}{c}{AH} & \multicolumn{1}{c}{EP} \\ \cline{2-5} 
\multirow{4}{*}{\textbf{Task}} & \multirow{2}{*}{MR} & Statement coverage & 100\% & 100\% \\
 &  & Branch coverage & 88.1\% & 94.4\% \\ \cline{2-5} 
 & \multirow{2}{*}{BSK} & Statement coverage & 100\% & 96.5\% \\
 &  & Branch coverage & 100\% & 94.4\% \\ \cline{2-5} 
\end{tabular}
\end{table}

Given the coverage values, it is reasonable to assume that both test suites give the same or strongly correlated results. Simple correlation analysis can be used to assess convergent validity \cite[p.67]{Oxford2006}. Table~\ref{tab:correlations} shows the results. All correlations are large ( $r > 0.5$, according to Cohen \cite{Cohen1988}), with the only exception of QLTY at PT ($r = 0.41$, quite close to 0.5), and statistically significant (which is remarkable given the limited sample sizes). At the outset, AH and EP test suites seem to provide similar measures; in the case of EC, to a large extent.

\begin{table}[htb]
\small
\centering
\caption{Correlations between \textit{ad-hoc} and \textit{equivalence partitioning} measures for PT and EC}
\label{tab:correlations}
%\resizebox{\columnwidth}{!}{%
<<correlation_table, results=tex>>=
corrdata <- matrix(0, ncol =1, nrow = 11)

PT_QLTY <- rcorr(expdata[expdata$Experiment == "PT" & expdata$Instrument == "AH",]$QLTY,
                 expdata[expdata$Experiment == "PT" & expdata$Instrument == "EP",]$QLTY)

PT_PROD <- rcorr(expdata[expdata$Experiment == "PT" & expdata$Instrument == "AH",]$PROD,
                 expdata[expdata$Experiment == "PT" & expdata$Instrument == "EP",]$PROD)

EC_QLTY <- rcorr(expdata[expdata$Experiment == "EC" & expdata$Instrument == "AH",]$QLTY,
                 expdata[expdata$Experiment == "EC" & expdata$Instrument == "EP",]$QLTY)

EC_PROD <- rcorr(expdata[expdata$Experiment == "EC" & expdata$Instrument == "AH",]$PROD,
                 expdata[expdata$Experiment == "EC" & expdata$Instrument == "EP",]$PROD)

corrdata[1] <- "\\begin{tabular}{llrr}"
corrdata[2] <- ""
corrdata[3] <- "\\multicolumn{1}{c}{Experiment} &"
corrdata[4] <- "\\multicolumn{1}{c}{Variable} &"
corrdata[5] <- "\\multicolumn{1}{c}{$r$} &"
corrdata[6] <- "\\multicolumn{1}{c}{$p-value$} \\\\ \\hline"
corrdata[7] <- paste("\\multirow{2}{*}{PT} & QLTY & ", round(PT_QLTY$r[2], digits=2), " & ", ifelse(PT_QLTY$P[2]<0.001, "\\textless 0.001", round(PT_QLTY$P[2], digits=2)), " \\\\", sep = "")
corrdata[8] <- paste("& PROD & ", round(PT_PROD$r[2], digits=2), " & ", ifelse(PT_PROD$P[2]<0.001, "\\textless 0.001", round(PT_PROD$P[2], digits=2)), " \\\\ \\hline", sep = "")
corrdata[9] <- paste("\\multirow{2}{*}{EC} & QLTY & ", round(EC_QLTY$r[2], digits=2), " & ", ifelse(EC_QLTY$P[2]<0.001, "\\textless 0.001", round(EC_QLTY$P[2], digits=2)), " \\\\", sep = "")
corrdata[10]<- paste("& PROD & ", round(EC_PROD$r[2], digits=2), " & ", ifelse(EC_PROD$P[2]<0.001, "\\textless 0.001", round(EC_PROD$P[2], digits=2)), " \\\\ \\hline", sep = "")
corrdata[11]<-"\\end{tabular}"
write.table(corrdata,sep="", col.names = FALSE, row.names = FALSE, quote=FALSE)
@
%}
\end{table}

\subsection{Problem detection}\label{sec:problem}

PT and EC experiments were analyzed as recommended by Vegas et al. \cite{Vegas2016}, i.e., using a mixed model where \textit{Treatment}, \textit{Task} and \textit{Group} are fixed factors, and \textit{Subject} is a random factor embedded within each \textit{Group}. The analysis model using the \textit{lme4} package \cite{lme4} is:

\begin{equation}
\label{eq:analysis-model}
Y \sim Treatment + Task + Group + ( 1 | Subject )   
\end{equation}

where $Y$ can be QLTY or PROD. We will restrict the discussion to the QLTY response variable, but the comments below match PROD's behavior as well. Tables~\ref{tab:qlty-analysis-pt} and \ref{tab:qlty-analysis-ec} show the analysis results for QLTY at PT and EC, respectively. The numbers between parentheses represent the \textit{standard error} of the \textit{fixed effect} located to its left. The degree of statistical significance is reported using asterisks. The differences between the AH and EP test suites are substantial:

\begin{itemize}

\item The \textit{Task} effect \textbf{reverses depending on the test case definition strategy}. For AH, the effect is negative whereas, for EP, the effect is positive. The changes are dramatic in PT's QLTY (from -24.47 to 20.99 percentage points). Differences are statistically significant both for PT and EC experiments.

\item The \textit{Group} effect is \textbf{positive for AH, and void for EP}. The analysis does not give statistically significant results in this case.

\item Fixed effects are \textbf{larger for AH than EP} regardless of the variable (the \textit{Task} at EC is the exception). Standard deviations are also \textbf{larger for AH than EP} in all cases.

\end{itemize}

There is just one coincidence between the AH and EP measurements:

\begin{itemize}

\item The \textit{Treatment} (ITLD vs. TDD) \textbf{is largely unaffected}. The AH and EP test suites give different values, but the sign and the statistical significance is preserved. 
 
\end{itemize}

<<results=hide>>=
lm1 <- lmer(QLTY ~  1 +
              Treatment +
              Task +
              Group +
              (1 | Subject),
            data = expdata[expdata$Experiment == "PT" &
                             expdata$Instrument == "AH", ])

lm2 <- lmer(QLTY ~  1 +
              Treatment +
              Task +
              Group +
              (1 | Subject),
            data = expdata[expdata$Experiment == "PT" &
                             expdata$Instrument == "EP", ])
lm3 <- lmer(QLTY ~  1 +
              Treatment +
              Task +
              Group +
              (1 | Subject),
            data = expdata[expdata$Experiment == "EC" &
                             expdata$Instrument == "AH", ])

lm4 <- lmer(QLTY ~  1 +
              Treatment +
              Task +
              Group +
              (1 | Subject),
            data = expdata[expdata$Experiment == "EC" &
                             expdata$Instrument == "EP", ])

mylm1 <- extract(lm1, 
                 #include.aic = FALSE, 
                 include.bic = FALSE, 
                 include.loglik = FALSE,
                 #include.nobs = FALSE,
                 include.groups = FALSE)

mylm2 <- extract(lm2, 
                 #include.aic = FALSE, 
                 include.bic = FALSE, 
                 include.loglik = FALSE,
                 #include.nobs = FALSE,
                 include.groups = FALSE)

mylm3 <- extract(lm3,
                 #include.aic = FALSE,
                 include.bic = FALSE,
                 include.loglik = FALSE,
                 #include.nobs = FALSE,
                 include.groups = FALSE)

mylm4 <- extract(lm4,
                 #include.aic = FALSE,
                 include.bic = FALSE,
                 include.loglik = FALSE,
                 #include.nobs = FALSE,
                 include.groups = FALSE)
@

\begin{table}[htb]
\small
\centering
\caption{Analysis of the QLTY response variable for PT}\label{tab:qlty-analysis-pt}
<<results=tex>>=
texreg(l = list(mylm1, mylm2),
       single.row = TRUE,
       custom.model.names = c("AH","EP"),
       digits = 2,
       leading.zero = FALSE,
       caption.above = TRUE,
       table = FALSE)
@
\end{table}

\begin{table}[htb]
\small
\centering
\caption{Analysis of the QLTY response variable for EC}\label{tab:qlty-analysis-ec}
<<results=tex>>=
texreg(l = list(mylm3, mylm4),
       single.row = TRUE,
       custom.model.names = c("AH","EP"),
       digits = 2,
       leading.zero = FALSE,
       caption.above = TRUE,
       table = FALSE)
@
\end{table}

We expected some disagreement between AH and EP, but not such large discrepancies. 

\begin{framed}

In practice, it implies that the experiment outcomes change depending on the test suites used as a measuring instrument, to the point of obtaining contradictory results.

\end{framed}







